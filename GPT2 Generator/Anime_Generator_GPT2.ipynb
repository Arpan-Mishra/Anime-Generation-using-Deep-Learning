{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Anime Generator - GPT2",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "2fYMUBtthNkq",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 356
        },
        "outputId": "456c177b-43d3-4ab5-c42d-62d6cb53d0c0"
      },
      "source": [
        "!pip install transformers"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.6/dist-packages (3.0.1)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from transformers) (2.23.0)\n",
            "Requirement already satisfied: sentencepiece!=0.1.92 in /usr/local/lib/python3.6/dist-packages (from transformers) (0.1.91)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.6/dist-packages (from transformers) (2019.12.20)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.6/dist-packages (from transformers) (20.4)\n",
            "Requirement already satisfied: dataclasses; python_version < \"3.7\" in /usr/local/lib/python3.6/dist-packages (from transformers) (0.7)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.6/dist-packages (from transformers) (3.0.12)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from transformers) (1.18.5)\n",
            "Requirement already satisfied: tokenizers==0.8.0-rc4 in /usr/local/lib/python3.6/dist-packages (from transformers) (0.8.0rc4)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.6/dist-packages (from transformers) (4.41.1)\n",
            "Requirement already satisfied: sacremoses in /usr/local/lib/python3.6/dist-packages (from transformers) (0.0.43)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (1.24.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (2.9)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (2020.6.20)\n",
            "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.6/dist-packages (from packaging->transformers) (2.4.7)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from packaging->transformers) (1.12.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (7.1.2)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (0.15.1)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FRkkVm1eXdNr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import transformers"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "27x77PXhWuHV",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        },
        "outputId": "2489309f-72c6-4083-bf4b-43b996348925"
      },
      "source": [
        "batch_size = 10\n",
        "model_path = 'gpt2_epoch5.bin'\n",
        "max_seq_len = 300\n",
        "epochs = 5\n",
        "data_path = '/content/eda-data.csv'\n",
        "tokenizer = transformers.GPT2Tokenizer.from_pretrained('gpt2')\n",
        "model = transformers.GPT2LMHeadModel.from_pretrained('gpt2')"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Some weights of GPT2LMHeadModel were not initialized from the model checkpoint at gpt2 and are newly initialized: ['h.0.attn.masked_bias', 'h.1.attn.masked_bias', 'h.2.attn.masked_bias', 'h.3.attn.masked_bias', 'h.4.attn.masked_bias', 'h.5.attn.masked_bias', 'h.6.attn.masked_bias', 'h.7.attn.masked_bias', 'h.8.attn.masked_bias', 'h.9.attn.masked_bias', 'h.10.attn.masked_bias', 'h.11.attn.masked_bias', 'lm_head.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "faXOwfpzhKQz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "import re\n",
        "\n",
        "def remove_source(text):\n",
        "    cln_text = text\n",
        "    if '(Source' in cln_text:\n",
        "        cln_text,_,_ = cln_text.partition('(Source')\n",
        "    elif '[Written ' in cln_text:\n",
        "        cln_text,_,_ = cln_text.partition('[Written')\n",
        "        \n",
        "    return cln_text\n",
        "\n",
        "def clean_synopsis(data):\n",
        "    # removing hentai and kids tags\n",
        "    data = data[(data.Hentai != 1) & (data.Kids != 1)]\n",
        "    synopsis = data.synopsis\n",
        "    synopsis = synopsis.apply(lambda x: str(x))\n",
        "\n",
        "    # removing very small synopsis\n",
        "    synopsis = synopsis.apply(lambda x: x if ((len(str(x).strip().split())<=300) and len(str(x).strip().split())>30  ) else -1)\n",
        "    synopsis = synopsis[synopsis!=-1]\n",
        "    \n",
        "    # removing source text\n",
        "    synopsis = synopsis.apply(lambda x: remove_source(x))\n",
        "    \n",
        "    # removing japanese characters\n",
        "    synopsis = synopsis.apply(lambda x: re.sub(\"([^\\x00-\\x7F])+\",\" \",x))\n",
        "    \n",
        "    # remove symbols\n",
        "    rx = re.compile('[&#/@`)(;<=\\'\"$%>]')\n",
        "    synopsis = synopsis.apply(lambda x: rx.sub('',x))\n",
        "    synopsis = synopsis.apply(lambda x: x.replace('>',\"\"))\n",
        "    synopsis = synopsis.apply(lambda x: x.replace('`',\"\"))\n",
        "    synopsis = synopsis.apply(lambda x: x.replace(')',\"\"))\n",
        "    synopsis = synopsis.apply(lambda x: x.replace('(',\"\"))\n",
        "    \n",
        "\n",
        "    # removing adaptation animes (some relevant might get deleted but there aren`t a lot so we wont be affected as much)\n",
        "    synopsis = synopsis[synopsis.apply(lambda x: 'adaptation' not in str(x).lower())]    \n",
        "    synopsis = synopsis[synopsis.apply(lambda x: 'music video' not in str(x).lower())]\n",
        "    synopsis = synopsis[synopsis.apply(lambda x: 'based on' not in str(x).lower())]\n",
        "    synopsis = synopsis[synopsis.apply(lambda x: 'spin-off' not in str(x).lower())]\n",
        "    \n",
        "    return synopsis.reset_index(drop=True)\n",
        "\n",
        "\n",
        "class AverageMeter:\n",
        "    \"\"\"Computes and stores the average and current value\"\"\"\n",
        "    def __init__(self):\n",
        "        self.reset()\n",
        "\n",
        "    def reset(self):\n",
        "        self.val = 0\n",
        "        self.avg = 0\n",
        "        self.sum = 0\n",
        "        self.count = 0\n",
        "\n",
        "    def update(self, val, n=1):\n",
        "        self.val = val\n",
        "        self.sum += val * n\n",
        "        self.count += n\n",
        "        self.avg = self.sum / self.count\n"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EZ6sCWMhhZvn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch\n",
        "\n",
        "class AnimeDataset():\n",
        "    def __init__(self,data):\n",
        "        self.eos_tok = '<|endoftext|>'\n",
        "        synopsis = clean_synopsis(data)\n",
        "        synopsis = synopsis.apply(lambda x: str(x) + self.eos_tok)\n",
        "        self.synopsis = synopsis.tolist()\n",
        "        self.pad_tok = tokenizer.encode(['<|pad|>'])\n",
        "    def __getitem__(self,item):\n",
        "        synopsis = self.synopsis[item]\n",
        "        tokens = tokenizer.encode(synopsis)\n",
        "        mask = [1]*len(tokens)\n",
        "        \n",
        "        max_len = max_seq_len\n",
        "        if max_len>len(tokens):\n",
        "            padding_len = max_len - len(tokens)\n",
        "            tokens = tokens + self.pad_tok*padding_len\n",
        "            mask = mask + [0]*padding_len\n",
        "        else:\n",
        "            tokens = tokens[:max_len]\n",
        "            mask = mask[:max_len]\n",
        "        \n",
        "        if tokens[-1]!= tokenizer.encode(self.eos_tok)[0]:\n",
        "            tokens[-1] = tokenizer.encode(self.eos_tok)[0]\n",
        "        \n",
        "        return {'ids':torch.tensor(tokens,dtype = torch.long),\n",
        "                'mask': torch.tensor(mask,dtype = torch.long),\n",
        "                'og_synpsis':synopsis}\n",
        "    \n",
        "        \n",
        "        \n",
        "     \n",
        "    def __len__(self):\n",
        "        return len(self.synopsis)\n",
        "\n"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6mHk-qWfhiRC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from tqdm import tqdm\n",
        "import torch\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "def train_fn(model,dataloader,optimizer,scheduler,device):\n",
        "    model.train()\n",
        "    tk0 = tqdm(dataloader, total = len(dataloader), leave = True, position = 0)\n",
        "    train_loss = AverageMeter()\n",
        "    losses = []\n",
        "    for bi,d in enumerate(tk0):\n",
        "            \n",
        "        ids = d['ids'].to(device,dtype = torch.long)\n",
        "        mask = d['mask'].to(device,dtype = torch.long)\n",
        "        \n",
        "        loss,out = model(input_ids = ids, labels = ids, attention_mask  = mask)[:2]\n",
        "        \n",
        "        train_loss.update(loss.item())    \n",
        "        loss.backward()\n",
        "        losses.append(loss.item())\n",
        "        optimizer.step()\n",
        "        scheduler.step()\n",
        "        model.zero_grad()\n",
        "        tk0.set_postfix(loss = train_loss.avg)\n",
        "    return np.mean(losses)        "
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QFPY6EcMhru4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import pandas as pd\n",
        "from transformers import AdamW \n",
        "from transformers import get_linear_schedule_with_warmup\n",
        "import torch\n",
        "def run():\n",
        "    data = pd.read_csv(data_path)\n",
        "    dataset = AnimeDataset(data = data)\n",
        "    dataloader = torch.utils.data.DataLoader(dataset,batch_size=batch_size,shuffle=True)\n",
        "    \n",
        "    device = 'cuda'\n",
        "    model.to(device)\n",
        "    \n",
        "    optimizer = AdamW(model.parameters(),lr = 0.0001,weight_decay = 0.003)    \n",
        "    scheduler = get_linear_schedule_with_warmup(optimizer,\n",
        "                                                num_warmup_steps=10,num_training_steps = int(len(data)/batch_size * epochs))\n",
        "    \n",
        "    best_loss = 111111\n",
        "    for epoch in range(epochs):\n",
        "        loss = train_fn(model,dataloader,optimizer,scheduler,device)\n",
        "        if loss<best_loss:\n",
        "            best_loss = loss\n",
        "            torch.save(model.state_dict(),model_path)\n",
        "        torch.cuda.empty_cache\n",
        "    "
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eZCtiYapVdpP",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 101
        },
        "outputId": "02f8ef82-d2a3-42c5-d75e-66a38c38417d"
      },
      "source": [
        "run()"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 731/731 [18:22<00:00,  1.51s/it, loss=1.63]\n",
            "100%|██████████| 731/731 [18:21<00:00,  1.51s/it, loss=1.46]\n",
            "100%|██████████| 731/731 [18:21<00:00,  1.51s/it, loss=1.38]\n",
            "100%|██████████| 731/731 [18:21<00:00,  1.51s/it, loss=1.31]\n",
            "100%|██████████| 731/731 [18:19<00:00,  1.50s/it, loss=1.25]\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SN0LiSFFKtiM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def generate_text(input_text,device = 'cuda',max_len = 300):\n",
        "  pad_tok = tokenizer.encode(['<|pad|>'])[0]\n",
        "  model.load_state_dict(torch.load(model_path))\n",
        "  model.to(device)\n",
        "  model.eval()\n",
        "\n",
        "  input_ids = tokenizer.encode(input_text)\n",
        "  mask = [1]*len(input_ids)\n",
        "\n",
        "  padding_len = max_seq_len - len(input_ids)\n",
        "  \n",
        "  input_ids = input_ids #+ pad_tok*padding_len\n",
        "  mask = mask + [0]*padding_len\n",
        "\n",
        "  ids = torch.tensor(input_ids,dtype = torch.long).to(device).unsqueeze(0)\n",
        "  mask = torch.tensor(mask,dtype = torch.long).to(device).unsqueeze(0)\n",
        "  \n",
        "  #print(ids[0])\n",
        "  sample_out = model.generate(ids, min_length = 30,max_length=max_len, pad_token_id=pad_tok,\n",
        "                              top_p=0.85, early_stopping=True, do_sample=True, num_beams = 5, no_repeat_ngram_size = 2,num_return_sequences=1)\n",
        "  \n",
        "  print('Generated Text:\\n\\n',tokenizer.decode(sample_out[0],skip_special_tokens = True))\n"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_m31rYeg8gc3",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 87
        },
        "outputId": "78a53378-a696-46be-e8c2-bf9724c3659e"
      },
      "source": [
        "generate_text('When the night',device = 'cuda')"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Generated Text:\n",
            "\n",
            " When the night before a school festival, a mysterious girl suddenly appears in front of the entire school. She claims to be from the future, and she wants to take over the world. To do this, she has to use her powers to transform into a magical girl.\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}